### 1. Teach-DETR: Better Training DETR with Teachers
*IEEE Transactions on Pattern Analysis and Machine Intelligence 2022*

利用教师检测器的伪标签和置信度分数作为额外独立和并行的监督来训练学生检测器查询的输出。可以与各种流行的基于 DETR 的检测器集成。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008190458.png)
- **边界框是所有检测器结果的无偏表示，不会受到不同检测器框架差异的影响，因此它可以作为将教师知识转移到学生 DETR 的良好媒介。**
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008191233.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008191333.png)
由于二分匹配极大地影响了Teach-DETR的效率，auxiliary boxes数量越多，进行二分匹配所需的时间就越多。因此，在实践中，通过选择置信度分数高的框来将每个教师的auxiliary boxes数量控制在 50 以下。使用一对一匹配。

### 2. Cross-Domain Few-Shot Object Detection via  Enhanced Open-Set Object Detector
*ECCV2024*

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241010221531.png)

通过三个维度来全面捕捉跨领域数据集：
Style、
Inter-class variance (ICV)：用于衡量类别之间的差异。 ICV 值越高表示语义标签的识别越容易。像 COCO 这样的粗数据集通常具有较高的 ICV，而更细粒度的数据集则表现出较小的 ICV 值。
Indefinable Boundaries (IB)：反映了目标物体与其背景之间的混淆程度。更大的混乱给检测器带来了挑战。例如，在干净的背景下检测一个人相对简单，但识别珊瑚礁中的鱼则更具挑战性。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241010222143.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241010222207.png)

### 3. DETRDistill: A Universal Knowledge Distillation Framework for DETR-families
*IEEE/CVF International Conference on Computer Vision (ICCV) 2022*
跑代码

解决的**问题**：
1. logits-level distillation methods
基于卷积的检测器其预测框与特征图网格密切相关，因此自然地确保了教师与学生框预测的严格空间对应性。但是DETR从解码器生成的框预测是无序的，并且教师和学生之间的预测框之间不存在自然的一对一对应关系以进行逻辑级蒸馏。

2. Feature-level distillation approaches
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008203322.png)
由于卷积和transformer之间的特征生成机制不同，感兴趣对象的特征激活区域变化很大。如图 所示，基于卷积的检测器的活动区域几乎限制在真实框内，而 DETR 检测器进一步激活背景区域中的区域。因此，直接使用以前的特征级 KD 方法进行 DETR 可能不一定会带来性能提升，有时甚至会损害学生检测器。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008203556.png)


1. Hungarian-matching Logits Distillation
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241010222940.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241010223020.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008210520.png)
K 是decoder stages，教师模型的大量负面预测被忽略，我们认为这些预测是有价值的。

2. Target-aware Feature Distillation

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008212830.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008212928.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008212948.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008213018.png)

目标感知质量得分作为一个指标来指导哪个查询应该对知识蒸馏中的 KD 损失做出更多贡献。

3. Query-prior Assignment Distillation
解码器中的query通常是随机初始化的，query可能会在不同的训练时期分配给不同的对象，从而导致匹配不稳定和收敛速度慢。
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220036.png)

Overall loss：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220109.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220326.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220423.png)

简单地将教师的解码器分组，如图所示，以处理阶段不匹配问题


![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220555.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241008220849.png)

### 4. KD-DETR: Knowledge Distillation for Detection Transformer with Consistent Distillation Points Sampling
*Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024*

为了解决 DETR 中缺乏一致蒸馏点的问题，本文提出了一种具有一致蒸馏点采样的 DETR 通用知识蒸馏范例（KD-DETR）。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105530.png)
General Sampling with Random Initialized Queries：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105630.png)

Specific Sampling with Teacher Queries：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105719.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105747.png)


Foreground Rebalance Weight：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105800.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009105857.png)

推广到多项蒸馏（Heterogeneous Distillation）：
CNN 检测器中的锚点和 DETR 中的对象查询都代表图像上的某些位置并共享空间一致性。首先用 CNN 检测器中的锚点坐标构造一组蒸馏点 q ̃ ，然后将它们转换为 DETR 中对象查询的公式。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009110059.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009110203.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009110415.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009110506.png)

### 5. DAB-DETR: DYNAMIC ANCHOR BOXES  ARE BETTER QUERIES FOR DETR
*ICLR 2022*

通过在decoder中引入Learnable Anchors来加速DETR的收敛速度和提高检测精度。
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009205537.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009205537.png)


![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009211949.png)

a self-attention module and a cross-attention module, which are used for query updating and feature probing respectively.

根据宽度和高度调制高斯核注意力：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009212312.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009212326.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009212338.png)
这种调制的位置注意力有助于提取不同宽度和高度的物体的特征。
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241009212416.png)