### 1. Domain Incremental Object Detection Based on Feature Space Topology Preserving Strategy
*IEEE Transactions on Circuits and Systems for Video Technology 2024*
baseline：YOLOv5s

在域增量学习过程中保留特征空间的拓扑结构，从而减轻灾难性遗忘问题。首先使用自组织映射（self-organizing map，SOM）对特征空间拓扑进行建模，然后根据SOM节点的质心向量形成一组锚定图像来记忆特征空间拓扑，最后开发拓扑保持损失函数用于惩罚后续 IL 会话期间特征空间的拓扑变化。 SOM 是一种无监督学习方法，它将高维空间映射到离散的二维网格。
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918201623.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918201845.png)

SOM的输入是 $\mathbf{F}^t = \{f(\mathbf{x};\theta^t)|\mathbf{x} \in \mathbf{X}^t\}$  

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918203835.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918203857.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918204743.png)

在SOM对特征空间的拓扑结构建模后，选取对应的两组锚定图像集：
1. Image-Level Anchor Image Set：

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918204636.png)
2. Bounding Box-Level Anchor Image Set：

$${\mathbf{F}}_{l}^{t} = \{\bar{\mathbf{f}}_{ij}^{t}|\bar{\mathbf{f}}_{ij}^{t} = \bar{f}_{ca}(\mathbf{x}_{i}^{t},\mathbf{o}_{ij}^{t};\theta^{t}),\mathbf{o}_{ij}^{t} \in \mathbf{x}_{i}^{t},\mathbf{x}_{i}^{t} \in\\\mathbf{X}^{t}, \Gamma(\mathbf{o}_{ii}^{t})=l\}$$

Anchor Loss：
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918210215.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918205820.png)


## 2. Gradual Divergence for Seamless Adaptation: A Novel Domain Incremental Learning Method
*International Conference on Machine Learning 2024*

提出了一种新颖的领域增量学习方法，可以有效地将新领域的表示适应到先前领域跨越的特征空间中。
利用编码器 $g$ 从输入图像中提取语义上有意义的表示，并使用双分类器 $f_{1}$ 和 $f_{2}$ 将这些表示投影到类分布。为了学习更通用和更稳健的表示，并强制两个分类器具有不同的决策机制，在第一个分类器 $f_{1}$中采用交叉熵损失，在第二个分类器 $f_{2}$中采用监督对比损失。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918161755.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918161947.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162028.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162054.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162110.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162127.png)

对抗学习的策略，三个阶段：
- DIVERGENCE（发散）
	冻结编码器 $g$ 最大化两个分类器之间的差异，以便它们能够识别来自新任务的传入样本，这些样本的表示不位于学习表示所跨越的空间中。
	![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162501.png)
- ADAPTATION（适应）
	调整编码器 $g$，冻结分类器 f1 和 f2 ，最小化它们的预测之间的差异来学习领域的中间表示。
	![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162705.png)
- REFINEMENT（细化）
	细化阶段的目标是细化编码器和分类器，以有效地将新的任务信息与先前学习的知识整合起来，使得学习到的整合表示空间和决策边界对于迄今为止看到的所有任务都表现良好。
	![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20240918162726.png)