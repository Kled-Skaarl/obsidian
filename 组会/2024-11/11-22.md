### 1. Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116183842.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116183930.png)

现有的基于 LoRA 的方法依赖于训练期间任务边界的知识，因为在每个新任务开始时会初始化一对新的 LoRA 参数。在无任务OCL中，数据连续流动，没有明确的任务边界，并且没有关于任务开始或结束的信息。这就需要一种机制来确定何时初始化新的 LoRA 参数。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116184411.png)
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116184447.png)

### 2. Who Needs Features? On the Surprising Effectiveness of Attention Transfer for Vision Transformers
*NeurIPS 2024*

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211022.png)

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211256.png)

注意力转移尽管简单，但非常有效。
![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211106.png)

- 注意力复制可以在很大程度上缩小从头训练到全重量微调之间的差距
- 注意力蒸馏可以匹配微调性能


复制 Q 使模型能够灵活地偏离教师注意力图并使用更适合下游任务的注意力模式

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211608.png)


![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211447.png)

表 3 显示 Q 蒸馏的效果明显更差。我们假设这是因为学生在学习 Q 时更难学习有用的键 K，并且因为注意力蒸馏已经可以灵活地调整其注意力图以适应下游任务。

![](https://cdn.jsdelivr.net/gh/Kled-Skaarl/Picturebed@master/notes/20241116211834.png)


